{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RATING PREDICTION PROJECT:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PROBLEM STATEMENT:\n",
    "**We have a client who has a website where people write different reviews for technical products. \n",
    "Now they are adding a new feature to their website i.e. The reviewer will have to add stars(rating) \n",
    "as well with the review. The rating is out 5 stars and it only has 5 options available 1 star, 2 stars, \n",
    "3 stars, 4 stars, 5 stars. Now they want to predict ratings for the reviews which were written in the \n",
    "past and they don’t have a rating. So, we have to build an application which can predict the rating \n",
    "by seeing the review.**\n",
    "\n",
    "### 1)Data Collection Phase:\n",
    "**You have to scrape at least 20000 rows of data. You can scrape more data as well, it’s up to you. \n",
    "more the data better the model\n",
    "In this section you need to scrape the reviews of different laptops, Phones, Headphones, smart \n",
    "watches, Professional Cameras, Printers, Monitors, Home theater, Router from different e\u0002commerce websites.\n",
    "Basically, we need these columns**\n",
    "- **1) reviews of the product.**\n",
    "- **2) rating of the product.**\n",
    "\n",
    "**You can fetch other data as well, if you think data can be useful or can help in the project.It \n",
    "completely depends on your imagination or assumption.**\n",
    "\n",
    "### 2)Model Building Phase:\n",
    "**After collecting the data, you need to build a machine learning model. Before model building do \n",
    "all data preprocessing steps involving NLP. Try different models with different hyper parameters \n",
    "and select the best model.**\n",
    "**Follow the complete life cycle of data science. Include all the steps like**\n",
    "- **1. Data Cleaning**\n",
    "- **2. Exploratory Data Analysis**\n",
    "- **3. Data Preprocessing**\n",
    "- **4. Model Building**\n",
    "- **5. Model Evaluation**\n",
    "- **6. Selecting the best model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Necessary Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import scipy as stats\n",
    "\n",
    "# To remove outliers\n",
    "from scipy.stats import zscore\n",
    "\n",
    "#importing nltk libraries\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import string\n",
    "from nltk import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import FreqDist\n",
    "\n",
    "# Evaluation Metrics\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.metrics import roc_curve,accuracy_score,roc_auc_score,hamming_loss, log_loss\n",
    "\n",
    "# Warning\n",
    "import warnings\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Scrapped Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading csv file \n",
    "df = pd.read_csv(\"Ratings_pred\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have imported the collected data which was in csv format and stored it as a dataframe. We can see the first 5 and last 5 observations of the dataset and it looks good also we have all string valued columns. In this perticular dataset we have about 114491 rows and 3 columns Unnamed: 0 is the index column of csv file so let's drop that column. Since **Ratings** is my target column and it is a categorical column with 5 categories so this problem is a **Multi Classification Problem**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Information:\n",
    "\n",
    "- Review_Title : Title of the Review.\n",
    "- Review_Text : Text Content of the Review.\n",
    "- Ratings : Ratings out of 5 stars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis [EDA]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Unnamed: 0 is the index column of csv file so let's drop that column as it will not help us in our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping unnecessary column\n",
    "df.drop(columns = 'Unnamed: 0',inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the shape of the dataset\n",
    "print(\"There are {} Rows and {} Columns in the dataset\".format(df.shape[0], df.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there are 114491 rows and 3 columns in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the column names in the dataset\n",
    "print(\"Columns present in the dataset are:\\n\",df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So above 3 are the column names in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the info of the dataset\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By observing the info we can say that there are some null values in the dataset and all the columns are of object data type which means all the entries are string entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for null values\n",
    "print(\"Null values in the dataset: \\n\", df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have a huge number of nan values in the dataset. Let's replace them using imputation methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize the null values clearly in dataset using heat map\n",
    "sns.set(rc={'figure.figsize':(11.8,8.27)})\n",
    "sns.heatmap(data=df.isnull())\n",
    "plt.xticks(rotation=90, fontsize=10)\n",
    "plt.yticks(rotation=0, fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can clear have a look on null values by using visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replacing nan values using imputation method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the mode of Review_Title column\n",
    "df[\"Review_Title\"].mode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the highest occuring Review_Title is Wonderful, we have to replace the nan values in Review_Title column with it's mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the mode of Review_Text column\n",
    "df[\"Review_Text\"].mode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the highest occuring Review_Text is Good, we have to replace the nan values in Review_Text column with it's mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the value counts of Ratings column\n",
    "df.Ratings.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the highes count in Ratings column is for 5 followed by 5.0 out of 5 starts and they both are same so it is clear the mode for Ratings column is 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the mode\n",
    "df[\"Ratings\"].mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing naa values with there mode as all the columns are categorical\n",
    "df[\"Review_Title\"] = df[\"Review_Title\"].fillna(df[\"Review_Title\"].mode()[0])\n",
    "df[\"Review_Text\"] = df[\"Review_Text\"].fillna(df[\"Review_Text\"].mode()[0])\n",
    "df[\"Ratings\"] = df[\"Ratings\"].fillna(df[\"Ratings\"].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for null values again\n",
    "print(\"Null values in the dataset: \\n\", df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now successfully we have replaced all the nan values using imputation method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's have a look into target column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the unique value count of target column\n",
    "df['Ratings'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking the above entries in target column we came to know that we need to replace the string entries to there respective values(stars)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing the string entries in target column\n",
    "df['Ratings'] = df['Ratings'].replace('1.0 out of 5 stars',1)\n",
    "df['Ratings'] = df['Ratings'].replace('2.0 out of 5 stars',2)\n",
    "df['Ratings'] = df['Ratings'].replace('3.0 out of 5 stars',3)\n",
    "df['Ratings'] = df['Ratings'].replace('4.0 out of 5 stars',4)\n",
    "df['Ratings'] = df['Ratings'].replace('5.0 out of 5 stars',5)\n",
    "df['Ratings'] = df['Ratings'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the unique value count of target column again\n",
    "df['Ratings'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the target column looks good for our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's combine Review_Title and Review_Text to make a single column Review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining Review text and title\n",
    "df['Review'] = df['Review_Title'].map(str)+' '+df['Review_Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's have a look into our dataset\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have obtained Review from Review_Title and Review_Text let's drop Review_Title and Review_Text. If not they'll create multicolinearity issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping unnecessary column\n",
    "df.drop(columns = 'Review_Title',inplace = True)\n",
    "df.drop(columns = 'Review_Text',inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's have a look into our Review column and see first 2 entries how the data looks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking data of first row in Review column\n",
    "df['Review'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking data of second row in Review column\n",
    "df['Review'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By observing the Reviews we can say that there are many words, numbers, as well as punctuations which are not important for our predictions. So we need to do good text processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here I am defining a function to replace some of the contracted words to their full form and removing urls and some unwanted text \n",
    "def decontracted(text):\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"don’t\", \"do not\", text)\n",
    "    text = re.sub(r\"can't\", \"can not\", text)\n",
    "    text = re.sub(r\"im \", \"i am\", text)\n",
    "    text = re.sub(r\"yo \", \"you \",text)\n",
    "    text = re.sub(r\"doesn’t\", \"does not\",text)\n",
    "    text = re.sub(r\"n\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'s\", \" is\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'m\", \" am\", text)\n",
    "    text = re.sub(r\"<br>\", \" \", text)\n",
    "    text = re.sub(r'http\\S+', '', text) #removing urls\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing all words to there Lowercase\n",
    "df['Review'] = df['Review'].apply(lambda x : x.lower())\n",
    "\n",
    "df['Review'] = df['Review'].apply(lambda x : decontracted(x))\n",
    "\n",
    "# Removing punctuations\n",
    "df['Review'] = df['Review'].str.replace('[^\\w\\s]','')\n",
    "df['Review'] = df['Review'].str.replace('\\n',' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's have a look into our text again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking data of first row in Review column again\n",
    "df['Review'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking data of second row in Review column again\n",
    "df['Review'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the data looks far better than previous.And we have successfully removed punctuations and unwanted text from our text and lowercased all the text data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing StopWords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing stopwords\n",
    "stop = stopwords.words('english')\n",
    "df['Review'] = df['Review'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the text data again\n",
    "df['Review'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the text data again\n",
    "df['Review'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have removed all stop words from the text data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialising lemmatizer\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining functiom to convert nltk tag to wordnet tags\n",
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining function to lemmatize our text\n",
    "def lemmatize_sentence(sentence):\n",
    "    #tokenize the sentence & find the pos tag\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "    #tuple of (token, wordnet_tag)\n",
    "    wordnet_tagged = map(lambda x : (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "    lemmatize_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            lemmatize_sentence.append(word)\n",
    "        else:\n",
    "            lemmatize_sentence.append(lemmatizer.lemmatize(word,tag))\n",
    "    return \" \".join(lemmatize_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Review'] = df['Review'].apply(lambda x : lemmatize_sentence(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the text data again\n",
    "df['Review'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the text data again\n",
    "df['Review'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have removed the inflectional endings and left out with the base or dictionary form of a word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Normalization - Standardization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Noice removal\n",
    "def scrub_words(text):\n",
    "    #remove html markup\n",
    "    text = re.sub(\"(<.*?>)\", \"\", text)\n",
    "    #remove non-ascii and digits\n",
    "    text = re.sub(\"(\\\\W)\", \" \", text)\n",
    "    text = re.sub(\"(\\\\d)\", \"\", text)\n",
    "    #remove white space\n",
    "    text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Review'] = df['Review'].apply(lambda x : scrub_words(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the text data again\n",
    "df['Review'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the text data again\n",
    "df['Review'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally I have defined a function scrub_words for removing the noise from the text. It will remove any html markups, digits and white spaces from the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now We did all the text-processing steps and got required input for our model. We will get into Visualization part now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## i) Word Counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating column for word counts in the text\n",
    "df['Review_WordCount'] = df['Review'].apply(lambda x: len(str(x).split(' ')))\n",
    "df[['Review_WordCount','Review']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# density plot and histogram of Review word count\n",
    "sns.distplot(df['Review_WordCount'], hist = True, kde = True,\n",
    "            bins = int(180/5), color = 'y',\n",
    "            hist_kws = {'edgecolor':'black'},\n",
    "            kde_kws = {'linewidth':4})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By observing the histogram we can clearly see that most of our text is having the number of words in the range of 0 to 200, But some of the reviews are too lengthy which may act like outliers in our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ii) Character count:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating column for character counts in the text\n",
    "df['Review_ChaCount'] = df['Review'].str.len()\n",
    "df[['Review_ChaCount','Review']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# density plot and histogram of all character count\n",
    "sns.distplot(df['Review_ChaCount'], hist = True, kde = True,\n",
    "            bins = int(180/5), color = 'darkblue',\n",
    "            hist_kws = {'edgecolor':'black'},\n",
    "            kde_kws = {'linewidth':4})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above plot represents histogram for character count of Review text, which is quite similar to the histogram of word count."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Outliers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we know that some of the review are too lengthy, so i have to treat them as outliers and remove them using z_score method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the shape of the dataset\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying zscore to remove outliers\n",
    "from scipy import stats\n",
    "from scipy.stats import zscore\n",
    "z_score = zscore(df[['Review_WordCount']])\n",
    "abs_z_score = np.abs(z_score)\n",
    "filtering_entry = (abs_z_score < 3).all(axis = 1)\n",
    "df = df[filtering_entry]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great by removing the outliers we are loosing 1.3% of data which is very less and it is in acceptable range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting histograms for word count and character counts again after removing outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# density plot and histogram of Review word count\n",
    "sns.distplot(df['Review_WordCount'], hist = True, kde = True,\n",
    "            bins = int(180/5), color = 'y',\n",
    "            hist_kws = {'edgecolor':'black'},\n",
    "            kde_kws = {'linewidth':4})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# density plot and histogram of all character count\n",
    "sns.distplot(df['Review_ChaCount'], hist = True, kde = True,\n",
    "            bins = int(180/5), color = 'darkblue',\n",
    "            hist_kws = {'edgecolor':'black'},\n",
    "            kde_kws = {'linewidth':4})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After plotting histograms for word counts and character counts and after removing outliers we can see we are left out with good range of number of words and characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# iii) Top 30 most frequently occuring words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to plot most frequent terms\n",
    "def freq_words(x, terms = 30):\n",
    "    all_words = ' '.join([text for text in x])\n",
    "    all_words = all_words.split()\n",
    "    fdist = FreqDist(all_words)\n",
    "    words_df = pd.DataFrame({'word':list(fdist.keys()),\n",
    "                             'count':list(fdist.values())})\n",
    "    #selecting top 30 most freq words\n",
    "    d = words_df.nlargest(columns = 'count', n = terms)\n",
    "    plt.figure(figsize = (20,10))\n",
    "    ax = sns.barplot(data = d, x='count', y='word')\n",
    "    ax.set(ylabel = 'Word')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_words(df['Review'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By seeing the above plot we can see that Good, prodout, quality......are occurring frequently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# iv) Top 30 Rare words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to plot least frequent terms\n",
    "def rare_words(x, terms = 30):\n",
    "    all_words = ' '.join([text for text in x])\n",
    "    all_words = all_words.split()\n",
    "    fdist = FreqDist(all_words)\n",
    "    words_df = pd.DataFrame({'word':list(fdist.keys()),\n",
    "                             'count':list(fdist.values())})\n",
    "    #selecting top 30 most freq words\n",
    "    d = words_df.nsmallest(columns = 'count', n = terms)\n",
    "    plt.figure(figsize = (20,10))\n",
    "    ax = sns.barplot(data = d, x='count', y='word')\n",
    "    ax.set(ylabel = 'Word')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rare_words(df['Review'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above list of words are have rare occurance in Review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v) Word cloud:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "stopwords = set(STOPWORDS)\n",
    "def show_wordcloud(data, title = None):\n",
    "    wordcloud = WordCloud(\n",
    "                    background_color='white',\n",
    "                    stopwords = stopwords,\n",
    "                    max_words = 500,\n",
    "                    max_font_size = 40,\n",
    "                    scale = 3,\n",
    "                    random_state = 1).generate(str(data))\n",
    "    fig = plt.figure(1, figsize=(15,15))\n",
    "    plt.axis('off')\n",
    "    if title:\n",
    "        fig.suptitle(title, fontsize=20)\n",
    "        fig.subplots_adjust(top=2.3)\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's plot the loud words with Rating 1\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "df1=df['Review'][df['Ratings']==1]\n",
    "\n",
    "spam_cloud = WordCloud(width=700,height=500,background_color='white',stopwords = stopwords,max_words = 100,max_font_size = 80,scale = 3,random_state = 1).generate(' '.join(df1))\n",
    "\n",
    "plt.figure(figsize=(10,8),facecolor='r')\n",
    "plt.imshow(spam_cloud)\n",
    "plt.axis('off')\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's plot the loud words with Rating 2\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "df2=df['Review'][df['Ratings']==2]\n",
    "\n",
    "spam_cloud = WordCloud(width=700,height=500,background_color='white',stopwords = stopwords,max_words = 100,max_font_size = 80,scale = 3,random_state = 1).generate(' '.join(df1))\n",
    "\n",
    "plt.figure(figsize=(10,8),facecolor='b')\n",
    "plt.imshow(spam_cloud)\n",
    "plt.axis('off')\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's plot the loud words with Rating 3\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "df3=df['Review'][df['Ratings']==3]\n",
    "\n",
    "spam_cloud = WordCloud(width=700,height=500,background_color='white',stopwords = stopwords,max_words = 100,max_font_size = 80,scale = 3,random_state = 1).generate(' '.join(df1))\n",
    "\n",
    "plt.figure(figsize=(10,8),facecolor='g')\n",
    "plt.imshow(spam_cloud)\n",
    "plt.axis('off')\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's plot the loud words with Rating 4\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "df4=df['Review'][df['Ratings']==4]\n",
    "\n",
    "spam_cloud = WordCloud(width=700,height=500,background_color='white',stopwords = stopwords,max_words = 100,max_font_size = 80,scale = 3,random_state = 1).generate(' '.join(df1))\n",
    "\n",
    "plt.figure(figsize=(10,8),facecolor='y')\n",
    "plt.imshow(spam_cloud)\n",
    "plt.axis('off')\n",
    "plt.tight_layout(pad=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's plot the loud words with Rating 5\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "df5=df['Review'][df['Ratings']==5]\n",
    "\n",
    "spam_cloud = WordCloud(width=700,height=500,background_color='white',stopwords = stopwords,max_words = 100,max_font_size = 80,scale = 3,random_state = 1).generate(' '.join(df1))\n",
    "\n",
    "plt.figure(figsize=(10,8),facecolor='blue')\n",
    "plt.imshow(spam_cloud)\n",
    "plt.axis('off')\n",
    "plt.tight_layout(pad=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "- From the above plots we can clearly see the words which are indication of Reviewer's opinion on products.\n",
    "- Here most frequent words used for each Rating is displayed in the word cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the value counts of Ratings column\n",
    "df.Ratings.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building and Evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating feature and label\n",
    "x = df['Review']\n",
    "y = df['Ratings']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting text data into vectors using Tfidf Vectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using the n_gram tfidf vectorizer(Word vectors)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "word_vectorizer = TfidfVectorizer(\n",
    "                                sublinear_tf = True,\n",
    "                                strip_accents = 'unicode',\n",
    "                                analyzer = 'word',\n",
    "                                token_pattern = r'\\w{1,}',\n",
    "                                stop_words = 'english',\n",
    "                                ngram_range = (1,3),\n",
    "                                max_features = 100000)\n",
    "word_vectorizer.fit(x)\n",
    "train_word_features = word_vectorizer.transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Character vectorizer\n",
    "char_vectorizer = TfidfVectorizer(\n",
    "                                sublinear_tf = True,\n",
    "                                strip_accents = 'unicode',\n",
    "                                analyzer = 'char',\n",
    "                                stop_words = 'english',\n",
    "                                ngram_range = (2,6),\n",
    "                                max_features = 50000)\n",
    "char_vectorizer.fit(x)\n",
    "train_char_features = char_vectorizer.transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I will combine both word vectors and character vectors as input for our model\n",
    "from scipy.sparse import hstack\n",
    "train_features = hstack([train_char_features,train_word_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the data into train and test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting train and test data\n",
    "seed = 1\n",
    "x_train, x_test, y_train, y_test = train_test_split(train_features, y, test_size = 0.25, random_state = seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Balancing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets check the shapes of traning and test data\n",
    "print(\"x_train\", x_train.shape)\n",
    "print(\"x_test\", x_test.shape)\n",
    "print(\"y_train\", y_train.shape)\n",
    "print(\"y_test\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do oversmapling in order to make data balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the value counts of Ratings column\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the number of classes before fit\n",
    "from collections import Counter\n",
    "print(\"The number of classes before fit{}\".format(Counter(y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have maximum count 37633 for 5ratings hence will over sample mannually all the ratings to the mark 37633."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oversample and plot imbalanced dataset with SMOTE\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# transforming the dataset\n",
    "os=SMOTE(sampling_strategy = {1: 37633, 2: 37633, 3: 37633, 4: 37633, 5: 37633})\n",
    "x_train_ns,y_train_ns=os.fit_resample(x_train,y_train)\n",
    "\n",
    "print(\"The number of classes before fit{}\".format(Counter(y_train)))\n",
    "print(\"The number of classes after fit {}\".format(Counter(y_train_ns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have successfully balanced the data. Let's proceed with model building."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries for ML Algorithms\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB,BernoulliNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the algorithms\n",
    "rf = RandomForestClassifier()\n",
    "DTC = DecisionTreeClassifier()\n",
    "svc = LinearSVC()\n",
    "lr = LogisticRegression(solver='lbfgs')\n",
    "mnb = MultinomialNB()\n",
    "bnb = BernoulliNB()\n",
    "xgb = XGBClassifier(verbosity=0)\n",
    "lgb = LGBMClassifier()\n",
    "sgd = SGDClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a function to train and test the model with evaluation\n",
    "def BuiltModel(model):\n",
    "    print('*'*30+model.__class__.__name__+'*'*30)\n",
    "    model.fit(x_train_ns,y_train_ns)\n",
    "    y_pred = model.predict(x_train_ns)\n",
    "    pred = model.predict(x_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test,pred)*100\n",
    "\n",
    "    print(f\"Accuracy Score:\", accuracy)\n",
    "    print(\"---------------------------------------------------\")\n",
    "\n",
    "    #confusion matrix & classification report\n",
    "    \n",
    "    print(f\"CLASSIFICATION REPORT : \\n {classification_report(y_test,pred)}\")\n",
    "    print(f\"Confusion Matrix : \\n {confusion_matrix(y_test,pred)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Running multiple algorithms\n",
    "for model in [lr,svc,DTC,sgd,rf,xgb]:\n",
    "    BuiltModel(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have created 6 different classification algorithms. Great, among all these algorithms all are giving good accuracies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defning function cross_val to find cv score of models\n",
    "def cross_val(model):\n",
    "    print('*'*30+model.__class__.__name__+'*'*30)\n",
    "    scores = cross_val_score(model,train_features,y, cv = 3).mean()*100\n",
    "    print(\"Cross validation score :\", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in [lr,svc,DTC,sgd,rf,xgb]:\n",
    "    cross_val(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great all our algorithms are giving good cv scores.Among these algorithms I am selecting SGD Classifier as best fitting algorithm for our final model as it is giving least difference between accuracy and cv score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HyperParameter Tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's selects different parameters for tuning\n",
    "grid_params = {\n",
    "                'penalty':['l2','l1','elasticnet'],\n",
    "                'loss':['hinge','squared_hinge'],\n",
    "                'n_jobs':[-1,1]\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model with the given parameters using GridSearchCV\n",
    "GCV =  GridSearchCV(sgd, grid_params, cv = 3, verbose=10)\n",
    "GCV.fit(x_train_ns,y_train_ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the best parameters found by GridSearchCV\n",
    "GCV.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training our final model with above best parameters\n",
    "model = SGDClassifier(loss = 'squared_hinge', n_jobs = -1, penalty = 'l1')\n",
    "model.fit(x_train_ns,y_train_ns) #fitting data to model\n",
    "pred = model.predict(x_test)\n",
    "accuracy = accuracy_score(y_test,pred)*100\n",
    "\n",
    "# Printing accuracy score\n",
    "print(\"Accuracy Score :\", accuracy)\n",
    "\n",
    "# Printing Confusion matrix\n",
    "print(f\"\\nConfusion Matrix : \\n {confusion_matrix(y_test,pred)}\\n\")\n",
    "\n",
    "# Printing Classification report\n",
    "print(f\"\\nCLASSIFICATION REPORT : \\n {classification_report(y_test,pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After hyperparameter tuning we are unable to improved our model accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Saving:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(model,\"Ratings_RP.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally I have saved the model into .pkl file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion:\n",
    "### Key findings of the study :\n",
    "- In this project I have collected data of reviews and ratings for different products from amazon.in and flipkart.com. \n",
    "- Then I have done different text processing for reviews column and chose equal number of text from each rating class to eliminate problem of imbalance.\n",
    "- By doing different EDA steps I have analyzed the text. We have checked frequently occurring words in our data as well as rarely occurring words.\n",
    "- After all these steps I have built function to train different algorithms and using various evaluation metrics I have selected SGDClassifier for our final model.\n",
    "- Finally by doing hyperparameter tuning we got optimum parameters for our final model. And finally we got good accuracy score for our final model.\n",
    "\n",
    "### Limitations of this work and scope for the future work :\n",
    "As we know the content of text in reviews is totally depends on the reviewer and they may rate differently which is totally depends on that particular person. So it is difficult to predict ratings based on the reviews with higher accuracies. Still we can improve our accuracy by fetching more data and by doing extensive hyperparameter tuning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
