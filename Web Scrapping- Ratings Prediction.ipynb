{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WEB SCRAPING (RATING PREDICTION):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's scrap the data from flipkart.com and then will scrap the data from amazon.in. After scrapping the required data will concat the datas from flipkart and amazon. Let's strat the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessory libraries\n",
    "import selenium\n",
    "import pandas as pd\n",
    "import time\n",
    "import  requests\n",
    "from  bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapping Ratings and Reviews from Flipkart.com:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to the webdriver\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\nikit\\OneDrive\\Desktop\\DS Projects\\WS Selenium\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.flipkart.com/\"\n",
    "driver.get(url)\n",
    "time.sleep(2)\n",
    "#clicking on cancel buttopn \n",
    "driver.find_element_by_xpath(\"//div[@class='_2QfC02']/button\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srch_items = ['laptops', 'Phones','smart watches','Monitors']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = []\n",
    "review_text = []\n",
    "ratings = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap():    \n",
    "        for i in driver.find_elements_by_xpath(\"//div[@class='t-ZTKy']\"):\n",
    "            review_text.append(i.text)\n",
    "        for i in driver.find_elements_by_xpath(\"//p[@class='_2-N8zT']\"):\n",
    "            title.append(i.text)\n",
    "        for i in driver.find_elements_by_xpath(\"//p[@class='_2-N8zT']/../div\"):\n",
    "            ratings.append(i.text)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in srch_items:\n",
    "    #Find search bar\n",
    "    srchBar = driver.find_element_by_xpath(\"//div[@class='_3OO5Xc']/input\")\n",
    "    srchBar.clear()\n",
    "    srchBar.send_keys(i)\n",
    "    #clicking on search button\n",
    "    driver.find_element_by_xpath(\"//button[@class='L0Z3Pu']\").click()\n",
    "    time.sleep(3)\n",
    "    page = []\n",
    "    for i in driver.find_elements_by_xpath(\"//nav[@class='yFHi8N']/a\"):\n",
    "        page.append(i.get_attribute('href'))\n",
    "    for i in page[0:4]:\n",
    "        driver.get(i)\n",
    "        time.sleep(3)\n",
    "        items = driver.find_elements_by_xpath(\"//a[@class='_1fQZEK']\")\n",
    "        for i in items:\n",
    "            urls.append(i.get_attribute('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in urls:\n",
    "    driver.get(i)\n",
    "    time.sleep(2)\n",
    "    for _ in range(2):\n",
    "        driver.execute_script(\"window.scrollBy(0,6000)\")\n",
    "        time.sleep(1)\n",
    "        #clicking on all reviews\n",
    "    try:\n",
    "        btn=driver.find_element_by_xpath(\"//div[@class='_2c2kV-']/following::a\")\n",
    "        lnk = btn.get_attribute('href')\n",
    "        driver.get(lnk)\n",
    "        time.sleep(1)\n",
    "    except NoSuchElementException:\n",
    "        pass\n",
    "\n",
    "    scrap()        \n",
    "    try:\n",
    "        n_page=driver.find_elements_by_xpath(\"//nav[@class='yFHi8N']/a\")\n",
    "        np=[]\n",
    "        for i in n_page:\n",
    "            np.append(i.get_attribute('href'))\n",
    "        for i in np[0:18]:\n",
    "            driver.get(i)\n",
    "            time.sleep(1)\n",
    "            scrap()\n",
    "    except: continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ratings), len(review_text), len(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a dataframe\n",
    "data = list(zip(title,review_text,ratings))\n",
    "df2 = pd.DataFrame(data, columns = [\"Review_Title\",\"Reiew_Text\",\"Ratings\"])\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the data into csv file\n",
    "df2.to_csv(\"flipkrt1_reviews.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapping Ratings and Reviews from amazon.in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to the webdriver\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\POOJA C\\Downloads\\chromedriver_win32\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.amazon.in/\"\n",
    "driver.get(url)\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srch_items = ['laptops','phones','headphones','smart watches', 'Professional Cameras', 'Printers', 'Monitors', 'Home theater', 'Router']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = []\n",
    "review_text = []\n",
    "ratings = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in srch_items:\n",
    "    srchbar = driver.find_element_by_id(\"twotabsearchtextbox\")\n",
    "    srchbar.send_keys(i)\n",
    "    driver.find_element_by_id(\"nav-search-submit-button\").click()  #clicking on search button\n",
    "    time.sleep(1)\n",
    "    item_url = []\n",
    "    for i in driver.find_elements_by_xpath(\"//h2[@class='a-size-mini a-spacing-none a-color-base s-line-clamp-2']/a\"):\n",
    "        item_url.append(i.get_attribute('href'))\n",
    "    for i in item_url:\n",
    "        driver.get(i)\n",
    "        time.sleep(1)\n",
    "        for _ in range(2):\n",
    "            driver.execute_script(\"window.scrollBy(0,6000)\")\n",
    "            time.sleep(1)\n",
    "        #click on see all reviews\n",
    "        try:\n",
    "            driver.find_element_by_xpath(\"//a[@class='a-link-emphasis a-text-bold']\").click()\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        url_ = []\n",
    "        try:\n",
    "            two_str = driver.find_element_by_xpath(\"//table[@id='histogramTable']/tbody/tr[4]/td[2]/a\")\n",
    "            url_.append(two_str.get_attribute('href'))\n",
    "        except: pass\n",
    "        try:\n",
    "            three_str = driver.find_element_by_xpath(\"//table[@id='histogramTable']/tbody/tr[3]/td[2]/a\")\n",
    "            url_.append(three_str.get_attribute('href'))\n",
    "        except: pass\n",
    "        try:\n",
    "            one_str = driver.find_element_by_xpath(\"//table[@id='histogramTable']/tbody/tr[5]/td[2]/a\")\n",
    "            url_.append(one_str.get_attribute('href'))\n",
    "        except: pass\n",
    "        try:\n",
    "            fiv_str = driver.find_element_by_xpath(\"//table[@id='histogramTable']/tbody/tr[1]/td[2]/a\")\n",
    "            url_.append(fiv_str.get_attribute('href'))\n",
    "        except: pass\n",
    "        try:\n",
    "            four_str = driver.find_element_by_xpath(\"//table[@id='histogramTable']/tbody/tr[2]/td[2]/a\")\n",
    "            url_.append(four_str.get_attribute('href'))\n",
    "        except: pass\n",
    "        \n",
    "        for i in url_:\n",
    "            driver.get(i)\n",
    "            time.sleep(1)\n",
    "            for i in range(10):\n",
    "                ids = driver.find_elements_by_xpath(\"//div[@class='a-section a-spacing-none review-views celwidget']/div\")\n",
    "                comment_ids = []\n",
    "                for i in ids:\n",
    "                    comment_ids.append(i.get_attribute('id'))\n",
    "\n",
    "                for x in comment_ids:\n",
    "                    #Extract user ids from each user on a page\n",
    "                    try:\n",
    "                        review_title = driver.find_element_by_xpath('//*[@id=\"' + x +'\"]/div/div/div[2]/a[2]/span[1]')\n",
    "                        title.append(review_title.get_attribute(\"innerHTML\"))\n",
    "                    except NoSuchElementException:\n",
    "                        title.append('')\n",
    "\n",
    "                    try:\n",
    "                        rating = driver.find_element_by_xpath('//*[@id=\"' + x +'\"]/div/div/div[2]/a[1]/i/span[1]')\n",
    "                        ratings.append(rating.get_attribute(\"innerHTML\"))\n",
    "                    except NoSuchElementException:\n",
    "                        ratings.append('')\n",
    "\n",
    "                    try:\n",
    "                        text = driver.find_element_by_xpath('//*[@id=\"' + x +'\"]/div/div/div[4]/span/span')\n",
    "                        review_text.append(text.get_attribute(\"innerHTML\"))\n",
    "                    except NoSuchElementException:\n",
    "                        review_text.append('')\n",
    "                try:\n",
    "                    driver.find_element_by_xpath(\"//ul[@class='a-pagination']/li[2]/a\").click()\n",
    "                    time.sleep(3)\n",
    "                except : break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ratings), len(title), len(review_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a dataframe\n",
    "data = list(zip(title,review_text,ratings))\n",
    "df = pd.DataFrame(data, columns = [\"Review_title\",\"Reiew_text\",\"Ratings\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the data into csv file\n",
    "df.to_csv(\"amazon1_reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets load the flipkart reviews file\n",
    "flpkrt = pd.read_csv(\"flipkrt1_reviews.csv\")\n",
    "flpkrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets load the amazon reviews file\n",
    "df1 = pd.read_csv(\"amazon1_reviews.csv\")\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Since to concat the column names should be same in both the files so let me change the column names in both the files as if they are same.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the column name in amazon file similar to flipkart file\n",
    "df1.rename(columns = {'Review_title':'Review_Title'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the column name in amazon file similar to flipkart file\n",
    "df1.rename(columns = {'Reiew_text':'Review_Text'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the column name in flipkart file\n",
    "flpkrt.rename(columns = {'Reiew_Text':'Review_Text'}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Since unnamed :0 is the index column lets drop it in both the files.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping Unnamed: 0 in flpkrt file\n",
    "flpkrt.drop(columns = 'Unnamed: 0', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping Unnamed: 0 in amazon file\n",
    "df1.drop(columns = 'Unnamed: 0', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the files are ready to concat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concating both the files\n",
    "RP = pd.concat([df1, flpkrt],ignore_index=True)\n",
    "RP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the data is ready let's save it in csv format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the data into csv file\n",
    "RP.to_csv(\"Ratings_pred.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
